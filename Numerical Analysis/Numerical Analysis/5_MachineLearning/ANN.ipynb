{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioBolanos/AnalisisNumerico/blob/master/5_MachineLearning/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIKPAvdgvdZD"
      },
      "source": [
        "# Tomado de mplement a neural network from scratch with Python/Numpy â€” Backpropagation\n",
        "# Ali Mirzaei / Jan 19, 2019\n",
        "# https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
        "\n",
        "import numpy as np\n",
        "class NeuralNetwork(object):\n",
        "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
        "        assert(len(layers) == len(activations)+1)\n",
        "        self.layers = layers\n",
        "        self.activations = activations\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(len(layers)-1):\n",
        "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
        "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
        "    \n",
        "    def feedforward(self, x):\n",
        "        # return the feedforward value for x\n",
        "        a = np.copy(x)\n",
        "        z_s = []\n",
        "        a_s = [a]\n",
        "        for i in range(len(self.weights)):\n",
        "            activation_function = self.getActivationFunction(self.activations[i])\n",
        "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
        "            a = activation_function(z_s[-1])\n",
        "            a_s.append(a)\n",
        "        return (z_s, a_s)\n",
        "\n",
        "    def backpropagation(self,y, z_s, a_s):\n",
        "        dw = []  # dC/dW\n",
        "        db = []  # dC/dB\n",
        "        deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
        "        # insert the last layer error\n",
        "        deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
        "        # Perform BackPropagation\n",
        "        for i in reversed(range(len(deltas)-1)):\n",
        "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
        "        #a= [print(d.shape) for d in deltas]\n",
        "        batch_size = y.shape[1]\n",
        "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
        "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
        "        # return the derivitives respect to weight matrix and biases\n",
        "        return dw, db\n",
        "\n",
        "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
        "        # update weights and biases based on the output\n",
        "        e_print=0\n",
        "        for e in range(epochs): \n",
        "            i=0\n",
        "            while(i<len(y)):\n",
        "                x_batch = x[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                i = i+batch_size\n",
        "                z_s, a_s = self.feedforward(x_batch)\n",
        "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
        "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
        "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
        "                if e >= e_print:\n",
        "                  e_print = e_print+int(epochs/10)\n",
        "                  print(\"epoch = \"+str(e)+\", \"+\"loss = {}\".format(np.linalg.norm(a_s[-1]-y_batch) ))\n",
        "    @staticmethod\n",
        "    def getActivationFunction(name):\n",
        "        if(name == 'sigmoid'):\n",
        "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
        "        elif(name == 'linear'):\n",
        "            return lambda x : x\n",
        "        elif(name == 'relu'):\n",
        "            def relu(x):\n",
        "                y = np.copy(x)\n",
        "                y[y<0] = 0\n",
        "                return y\n",
        "            return relu\n",
        "        else:\n",
        "            print('Unknown activation function. linear is used')\n",
        "            return lambda x: x\n",
        "    \n",
        "    @staticmethod\n",
        "    def getDerivitiveActivationFunction(name):\n",
        "        if(name == 'sigmoid'):\n",
        "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
        "            return lambda x :sig(x)*(1-sig(x)) \n",
        "        elif(name == 'linear'):\n",
        "            return lambda x: 1\n",
        "        elif(name == 'relu'):\n",
        "            def relu_diff(x):\n",
        "                y = np.copy(x)\n",
        "                y[y>=0] = 1\n",
        "                y[y<0] = 0\n",
        "                return y\n",
        "            return relu_diff\n",
        "        else:\n",
        "            print('Unknown activation function. linear is used')\n",
        "            return lambda x: 1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWbo46mVes7A"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r9KVPpNClb2"
      },
      "source": [
        "my_nn = NeuralNetwork([2,1,1],activations=['linear', 'linear'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zqez9Tql4Ak"
      },
      "source": [
        "# my_nn.weights = np.array([[0.5, -0.5], [0]])\n",
        "# my_nn.biases = np.array([[ 1], [0]])\n",
        "# my_nn.activations = ['linear','linear']\n",
        "print(my_nn.weights)\n",
        "print(my_nn.biases)\n",
        "print(my_nn.activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ING1pg_shUg2"
      },
      "source": [
        "X = np.array([[0., 0.],[0., 1.],[1.,0.],[1.,1.]])\n",
        "# print(X.transpose())\n",
        "z_s, a_s = my_nn.feedforward(X.transpose())\n",
        "\n",
        "# print(z_s)\n",
        "# print(a_s)\n",
        "print(a_s[0])\n",
        "print(a_s[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpHTaNibtK--"
      },
      "source": [
        "import timeit\n",
        "\n",
        "y = np.array([[-1,1,1,1]])\n",
        "\n",
        "start = timeit.timeit()\n",
        "\n",
        "my_nn.train(X.transpose(), y, epochs=1000, batch_size=4, lr = .1)\n",
        "\n",
        "end = timeit.timeit()\n",
        "print(\"Elapsed time \"+str(end - start))\n",
        "\n",
        "z_s, a_s = my_nn.feedforward(X.transpose())\n",
        "\n",
        "# print(z_s)\n",
        "# print(a_s)\n",
        "print(a_s[0])\n",
        "print(a_s[2])\n",
        "print(my_nn.weights)\n",
        "print(my_nn.biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivym0qZKDhdk"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.set_aspect(1)\n",
        "i=0\n",
        "for dato in X:\n",
        "  c = 'r' if y[0][i] > 0 else 'b'\n",
        "  i+=1\n",
        "  ax.scatter(dato[0],dato[1], color=c)\n",
        "\n",
        "\n",
        "\n",
        "# print(z_s)\n",
        "# print(a_s)\n",
        "# print(a_s[0])\n",
        "# print(a_s[2])\n",
        "# xs = np.array([-1,2])\n",
        "# ys = np.array([2,-1])\n",
        "# ys1 = -(my_nn.weights[0][0][0]*xs+my_nn.biases[0][0][0])/my_nn.weights[0][0][0]\n",
        "# print(ys1)\n",
        "# ys= my_nn.weights[1][0][0]*ys1+my_nn.biases[1][0][0]\n",
        "# print(ys)\n",
        "# ax.plot(xs,ys,color='k')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKb1pl6r0Bc3"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "X_test = np.random.rand(400).reshape(200, 2)\n",
        "\n",
        "z_s, a_s = my_nn.feedforward(X_test.transpose())\n",
        "y_test = a_s[2]\n",
        "# print(X_test.transpose())\n",
        "# print(y_test)\n",
        "i=0\n",
        "for dato in X_test:\n",
        "  c = 'r' if y_test[0][i] > 0.5 else 'b'\n",
        "  i+=1\n",
        "  ax.scatter(dato[0],dato[1], color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4db9WZI-dL2u"
      },
      "source": [
        "# if __name__=='__main__':\n",
        "import matplotlib.pyplot as plt\n",
        "nn = NeuralNetwork([1, 10, 1],activations=['sigmoid', 'linear'])\n",
        "X = 2*np.pi*np.random.rand(5).reshape(1, -1)\n",
        "y = np.sin(X)\n",
        "print(X.size,X)\n",
        "print(y.size,y)\n",
        "z_s, a_s = nn.feedforward(X)\n",
        "# print(z_s)\n",
        "# print(a_s)\n",
        "# print(a_s[0])\n",
        "print(a_s[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS7JzzUM2UvJ"
      },
      "source": [
        "nn.train(X, y, epochs=1000, batch_size=64, lr = .1)\n",
        "z_s, a_s = nn.feedforward(X)\n",
        "#print(y, X)\n",
        "plt.scatter(X.flatten(), y.flatten())\n",
        "plt.scatter(X.flatten(), a_s[-1].flatten())\n",
        "plt.show()\n",
        "# print(z_s)\n",
        "# print(a_s)\n",
        "print(y)\n",
        "print(a_s[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1fXlGqwwXUQ"
      },
      "source": [
        "nn.train(X, y, epochs=1000, batch_size=64, lr = .1)\n",
        "z_s, a_s = nn.feedforward(X)\n",
        "#print(y, X)\n",
        "plt.scatter(X.flatten(), y.flatten())\n",
        "plt.scatter(X.flatten(), a_s[-1].flatten())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJnyc2rTgCjw"
      },
      "source": [
        "### imports\n",
        "# import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "### constant data\n",
        "x  = [[0.,0.],[1.,1.],[1.,0.],[0.,1.]]\n",
        "y_ = [[0.],[0.],[1.],[1.]]\n",
        "\n",
        "### induction\n",
        "# 1x2 input -> 2x3 hidden sigmoid -> 3x1 sigmoid output\n",
        "\n",
        "# Layer 0 = the x2 inputs\n",
        "x0 = tf.constant( x  , dtype=tf.float32 )\n",
        "y0 = tf.constant( y_ , dtype=tf.float32 )\n",
        "\n",
        "# Layer 1 = the 2x3 hidden sigmoid\n",
        "m1 = tf.Variable( tf.random_uniform( [2,3] , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "b1 = tf.Variable( tf.random_uniform( [3]   , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "h1 = tf.sigmoid( tf.matmul( x0,m1 ) + b1 )\n",
        "\n",
        "# Layer 2 = the 3x1 sigmoid output\n",
        "m2 = tf.Variable( tf.random_uniform( [3,1] , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "b2 = tf.Variable( tf.random_uniform( [1]   , minval=0.1 , maxval=0.9 , dtype=tf.float32  ))\n",
        "y_out = tf.sigmoid( tf.matmul( h1,m2 ) + b2 )\n",
        "\n",
        "\n",
        "### loss\n",
        "# loss : sum of the squares of y0 - y_out\n",
        "loss = tf.reduce_sum( tf.square( y0 - y_out ) )\n",
        "\n",
        "# training step : gradient decent (1.0) to minimize loss\n",
        "train = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
        "\n",
        "\n",
        "### training\n",
        "# run 500 times using all the X and Y\n",
        "# print out the loss and any other interesting info\n",
        "with tf.Session() as sess:\n",
        "  sess.run( tf.global_variables_initializer() )\n",
        "  for step in range(500) :\n",
        "    sess.run(train)\n",
        "\n",
        "  results = sess.run([m1,b1,m2,b2,y_out,loss])\n",
        "  labels  = \"m1,b1,m2,b2,y_out,loss\".split(\",\")\n",
        "  for label,result in zip(*(labels,results)) :\n",
        "    print (\"\")\n",
        "    print (label)\n",
        "    print (result)\n",
        "\n",
        "print (\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCsJX510gqQC"
      },
      "source": [
        "# TensorFlow y tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Librerias de ayuda\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kvxBMybgJg8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}